#+property: header-args:sh  :tangle ./etl.sh

manually tangle to =etl.sh= file with =C-c C-v t=  (=org-babel-tangle=)

#+begin_src sh
#!/bin/bash

# shell script configs
set -eu

echo "=microdata= preparation scripts for br-ibge-pns-2019\n"
#+end_src


#+begin_src sh
# urls where microdata and variables dictionary are located
mdata_url=ftp://ftp.ibge.gov.br/PNS/2019/Microdados/Dados/PNS_2019_20220525.zip
vdict_url=ftp://ftp.ibge.gov.br/PNS/2019/Microdados/Documentacao/Dicionario_e_input_20220530.zip

# source and directories where raw and processed data are to be stored
src_dir=/tmp/etl/br_ibge_pns_2019
raw_data=${src_dir}/raw
proc_data=${src_dir}/data
parq_data=${src_dir}/parquet
#+end_src


#+begin_src sh
create_temporary_directories(){

    echo "Creating temporary directory for raw data at $raw_data..."
    rm -rf ${raw_data}
    mkdir -p ${raw_data}

    echo "DONE.\nCreating temporary directory for processed data at $proc_data..."
    rm -rf ${proc_data}
    mkdir -p ${proc_data}
    mkdir -p ${parq_data}

    echo "DONE."
}
#+end_src


#+begin_src sh
fetch_microdata(){

    echo "Fetching the =microdata= from $mdata_url..."
    R --quiet --vanilla -e "curl::curl_download( '${mdata_url}', '${raw_data}/mdata.zip' )"

    echo '  DONE. Unziping the =microdata=...'
    unzip ${raw_data}/mdata.zip -d ${raw_data}

    echo '  DONE. Deleting the downloaded =microdata= zip file...'
    rm -rf ${raw_data}/mdata.zip
    echo 'DONE.'

}
#+end_src


#+begin_src sh
fetch_variables_dictionary(){

    echo "Fetching the dictionary of variables from $vdict_url..."
    R --quiet --vanilla -e "curl::curl_download( '${vdict_url}', '${raw_data}/vdict.zip' )"

    echo '  DONE. Unziping the data dictionary...'
    unzip ${raw_data}/vdict.zip -d ${raw_data}

    echo '  DONE. Deleting the downloaded data dictionary zip file...'
    rm -rf ${raw_data}/vdict.zip

}
#+end_src


#+begin_src R :tangle ./gnur_parse_variables_dictionary.R
## Extract-Transform-Load Script for the dictionary
## variables of National Health Survey 2019

library(readxl)   # read_excel
library(dplyr)    # mutate, filter, as_tibble, rename, fill, unique
library(readr)    # write_delim
library(tidyr)    # fill
library(stringr)  # str_pad, str_flatten, str_replace_all

src_dir <- '/tmp/etl/br_ibge_pns_2019/'
raw_data <- paste0(src_dir, 'raw/')
proc_data <- paste0(src_dir, 'data/')

dict <-
  read_excel(
    path = paste0(raw_data, 'dicionario_PNS_microdados_2019.xls'),
    sheet = 'dicionário pns',
    range = 'A5:G5224',
    col_types = 'text',
    col_names = FALSE
  ) %>%
  rename(
    pos = 1,
    len = 2,
    var = 3,
    q_code = 4,
    q_desc = 5,
    a_code = 6,
    a_desc = 7
  ) %>%
  mutate(
    part = '',
    module = '',
    .before = 'pos',
    q_desc = str_remove_all( q_desc, '[\r\n\"\\"]'),
    a_desc = str_remove_all( a_desc, '[\r\n\"\\"]')
  ) %>%
  fill(pos, len, var, q_code, q_desc) %>%
  as.data.frame()

aux_part <- ''
aux_module <- ''

for (i in seq_len(nrow(dict))){
  if (grepl('^parte|^variáv', dict[i,'pos'], ignore.case = TRUE)){
    aux_part <- dict[i,'pos']
  }
  if (grepl('^módulo', dict[i,'pos'], ignore.case = TRUE)){
    aux_module <- dict[i,'pos']
  }
  dict[i,'part'] <- aux_part
  dict[i,'module'] <- aux_module

  if (grepl('^variáv', dict[i,'part'], ignore.case = TRUE) ){
    dict[i,'module'] <- dict[i,'part']
  }
}

var_dict <-
  dict %>%
  filter(
    ## ignores lines not containing numbers
    !grepl('^parte|^variáve|^módulo|caso de mais|variáve', pos, ignore.case = TRUE),
    ) %>%
  mutate(
    var = tolower(var),
    ## arguments for command line tool 'cut'
    characters = paste0(pos, "-", as.numeric(pos) + as.numeric(len) -1)
  ) %>%
  transmute(part, module, q_code = var, q_desc, a_code, a_desc, characters)

## dictionary of variables
var_dict %>%
  select(part, module, q_code, q_desc, a_code, a_desc) %>%
  write_delim(
    file = paste0(src_dir, 'variables_dictionary.txt'),
    delim = '|',
    quote = 'none',
    col_names = TRUE
  )

## argument 'characters' of command line tool 'cut'
var_dict %>%
  select(characters) %>%
  unique() %>%
  summarise(
    args = str_flatten(characters, ",")
  ) %>%
  write_lines(
    file = paste0(proc_data, 'cut_characters_argument.txt')
  )

## column names for the pipe delimited (needed for .parquet files)
var_dict %>%
  select(q_code) %>%
  unique() %>%
  write_delim(
    file = paste0(proc_data, 'col_names.txt'),
    delim = '',
    quote = 'none',
    col_names = TRUE
  )

#+end_src


#+begin_src sh
parse_variables_dictionary_using_GNU_R(){

    echo 'Parsing the variables dictionary using GNU R'
    R CMD BATCH --quiet --vanilla ./gnur_parse_variables_dictionary.R
    rm *.Rout

    echo 'DONE.'
}
#+end_src


#+begin_src sh
prepare_microdata_for_data_ingestion(){

    echo 'Transforming microdata from fixed width format to delimited file...'

    # 'characters' argument from 'cut' function
    CHARACTERS_ARG=$(cat ${proc_data}/cut_characters_argument.txt)

    # from fixed width format to pipe (|) delimited text file
    cut \
        --output-delimiter=',' \
        --characters=$CHARACTERS_ARG \
        ${raw_data}/PNS_2019.txt > \
        ${proc_data}/microdata.txt

    echo 'DONE.'
}
#+end_src



#+begin_src R :tangle ./gnur_transform_microdata_into_parquet.R
## Converts microdata from text format to parquet using library(arrow)
library(arrow)  # open_dataset, write_dataset
library(readr)  # read_delim
library(purrr)  # map
library(dplyr)  # %>%, select

src_dir <- '/tmp/etl/br_ibge_pns_2019/'
raw_data <- paste0(src_dir, 'raw/')
proc_data <- paste0(src_dir, 'data/')
parq_data <- paste0(src_dir, 'parquet/')

col_names <-
  read_delim(
    file = paste0(proc_data, 'col_names.txt'),
    delim = '|',
    show_col_types = FALSE
  )
## adapted from https://stackoverflow.com/a/71305598
csv_schema <- schema(
  purrr::map( col_names$q_code, ~Field$create(name = .x, type = string()))
)

## reads comma delimited microdata using arrow package
microdata_pipe_delimited <-
  open_dataset(
    paste0(proc_data, 'microdata.txt'),
    format="csv",
    schema = csv_schema,
    col_names = TRUE,
    skip = 0
  )

## subset of variables used in the academic paper Deaf_DFLE
dfle_data <-
  microdata_pipe_delimited %>%
  select(
    ## survey design variables
    upa_pns, v0024, 
    v0028, v00281, v00282, v00283, # chosen respondent answers in the name of all persons of household
    ## variables for analyses
    j001,   # health_perception (for validation purposes only...)
    e001,   # work (for validation purposes only...)
    c006,   # gender
    c008,   # age
    q092,   # ever diagnosed with depression by a physician
    q11006, # ever diagnosed with anxiety by a physician
    g058,   # hearing impairment level
    g057,   # hearing impairment level even using hearning devices
    g05801, # knowdledge of Libras, the brazilian sign language
    v0015   # indicator for type of interview
  )

## saving microdata to disk in format 'parquet'
## for speed and smaller disk size
write_parquet(
  dfle_data, 
  sink = paste0(parq_data, 'microdata.parquet'),
  )
#+end_src


#+begin_src sh
convert_microdata_to_parquet_using_arrow_and_GNU_R(){

    echo 'Converting the processed microdata to .parquet format using GNU R and arrow library...'
    R CMD BATCH --quiet --vanilla ./gnur_transform_microdata_into_parquet.R
    rm *.Rout
    echo 'DONE.'
}
#+end_src



#+begin_src sh
remove_temporary_steps(){

    echo "Removing the temporary intermediate files"
    rm -rf ${raw_data}
    rm -rf ${proc_data}
    echo "DONE."
}
#+end_src



#+begin_src sh
main(){

    create_temporary_directories

    fetch_microdata

    fetch_variables_dictionary

    parse_variables_dictionary_using_GNU_R

    prepare_microdata_for_data_ingestion

    convert_microdata_to_parquet_using_arrow_and_GNU_R

    remove_temporary_steps

}

main

exit 0
#+end_src


# Local Variables:
# eval: (add-hook 'after-save-hook (lambda () (org-babel-tangle) ))
# End:
